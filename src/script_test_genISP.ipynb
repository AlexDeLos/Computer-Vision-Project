{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from dataset import CityscapesDataset\n",
    "from torchvision.datasets import Cityscapes\n",
    "import torch.nn.functional as F\n",
    "from GenISP import *\n",
    "from unet import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from tqdm import trange\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eleni\\Computer_Vision\\Computer-Vision-Project\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ALL CREDITS TO https://github.com/goldbattle/pytorch_unet/blob/master/script_train.py FOR DATASET CLASS\n",
    "# path = \"/Users/eleni/Cityscapes/\"\n",
    "path = R\"C:\\Users\\eleni\\Cityscapes\"\n",
    "# path = open(r\"C:\\Users\\eleni\\Downloads\\leftImg8bit_trainvaltest\\cityscapes\"\n",
    "img_data = CityscapesDataset(path, split='train', mode='fine')\n",
    "test_data = CityscapesDataset(path, split='test', mode='fine')\n",
    "batch_size = 8\n",
    "img_batch = torch.utils.data.DataLoader(img_data, batch_size=batch_size, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and stuff\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(device)\n",
    "\n",
    "# Set model parameters\n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "in_c = 3\n",
    "\n",
    "# Create the model\n",
    "in_channels = 3 # Raw RGB image\n",
    "hidden_channels = [16, 32, 128, 16, 64, 3]\n",
    "out_features = 3 # GenISP output channels\n",
    "PreNet = GenISPV2( in_channels, hidden_channels, out_features)\n",
    "UNet = UNet(in_c, img_data.num_classes, 64)\n",
    "# Net = nn.Sequential(PreNet,UNet)\n",
    "torch.no_grad()\n",
    "PreNet.to(device)\n",
    "UNet.to(device)\n",
    "train_loss = []\n",
    "UNet.load_state_dict(torch.load(R\"C:\\Users\\eleni\\Computer_Vision\\Computer-Vision-Project\\model.pth\", map_location=torch.device('cpu')))\n",
    "PreNet.load_state_dict(torch.load(R\"C:\\Users\\eleni\\Computer_Vision\\Computer-Vision-Project\\prenet_model.pth\", map_location=torch.device('cpu')))\n",
    "# UNet.load_state_dict(torch.load(\"model.pth\"),map_location=torch.device('cpu'))\n",
    "# PreNet.load_state_dict(torch.load(\"prenet_model.pth\"))\n",
    "\n",
    "# Since our goal here is to segment, we will use the cross entropy loss function and 4 classes (background, road, sky, car)\n",
    "criterion = nn.CrossEntropyLoss() # for segment loss. could also be reconstruction loss, then it is N1 loss \n",
    "num_classes = img_data.num_classes\n",
    "print(num_classes)\n",
    "optimizer = torch.optim.Adam(PreNet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9752273559570312, 0.9809913635253906, 0.9720268249511719, 0.9661941528320312, 0.9780044555664062, 0.9563674926757812, 0.9758338928222656, 0.9726715087890625, 0.96240234375, 0.9783897399902344, 0.9764633178710938, 0.9614334106445312, 0.9620018005371094, 0.9745941162109375, 0.9794120788574219, 0.9737129211425781, 0.9724235534667969, 0.9690513610839844, 0.9702033996582031, 0.9742355346679688, 0.9778327941894531, 0.975616455078125, 0.9759750366210938, 0.9712715148925781, 0.9594764709472656, 0.9781112670898438, 0.9801406860351562, 0.9735832214355469, 0.9750175476074219, 0.9736137390136719, 0.9738235473632812, 0.9675445556640625, 0.9763145446777344, 0.9691352844238281, 0.9689788818359375, 0.9597587585449219, 0.9739151000976562, 0.9818382263183594, 0.9732208251953125, 0.965576171875, 0.9760665893554688, 0.9710845947265625, 0.9775161743164062, 0.9680061340332031, 0.9720840454101562, 0.9761009216308594, 0.9782142639160156, 0.9759063720703125, 0.9684104919433594, 0.9730911254882812, 0.9566116333007812, 0.9642677307128906, 0.9766426086425781, 0.9765129089355469, 0.977996826171875, 0.9667930603027344, 0.972900390625, 0.9702377319335938, 0.9806556701660156, 0.9773979187011719, 0.9798393249511719, 0.9683494567871094, 0.9780349731445312, 0.9792137145996094, 0.9759521484375, 0.9694328308105469, 0.9698562622070312, 0.9564056396484375, 0.9756927490234375, 0.9699668884277344, 0.9660072326660156, 0.9849128723144531, 0.9657325744628906, 0.96185302734375, 0.9660224914550781, 0.9633979797363281, 0.9628944396972656, 0.964385986328125, 0.9730300903320312, 0.9724578857421875, 0.9681282043457031, 0.9715118408203125, 0.9472312927246094, 0.9793205261230469, 0.97216796875, 0.9716567993164062, 0.9808235168457031, 0.9706077575683594, 0.9553565979003906, 0.9729576110839844, 0.9766502380371094, 0.9726829528808594, 0.9699783325195312, 0.9789695739746094, 0.9652786254882812, 0.9713630676269531, 0.9644279479980469, 0.958526611328125, 0.9679298400878906, 0.9795875549316406, 0.9688568115234375, 0.9828834533691406, 0.9728317260742188, 0.973541259765625, 0.9760360717773438, 0.9811668395996094, 0.9763832092285156, 0.9720802307128906, 0.9771728515625, 0.9778366088867188, 0.9769325256347656, 0.9774513244628906, 0.9376678466796875, 0.9785537719726562, 0.9731254577636719, 0.9708023071289062, 0.9593315124511719, 0.9701576232910156, 0.9754714965820312, 0.9784507751464844, 0.979248046875, 0.9751091003417969, 0.9792747497558594, 0.9658393859863281, 0.9713058471679688, 0.9674911499023438, 0.9758110046386719, 0.9678382873535156, 0.9791297912597656, 0.9614219665527344, 0.9607086181640625, 0.9744987487792969, 0.9720726013183594, 0.9758033752441406, 0.9571304321289062, 0.9761123657226562, 0.9734573364257812, 0.9600639343261719, 0.9615974426269531, 0.9714775085449219, 0.9527702331542969, 0.9596290588378906, 0.9757423400878906, 0.9798965454101562, 0.9774627685546875, 0.9810981750488281, 0.9680900573730469, 0.975494384765625, 0.9779701232910156, 0.9780464172363281, 0.974945068359375, 0.9691581726074219, 0.9509506225585938, 0.94061279296875, 0.9714851379394531, 0.9744529724121094, 0.9730224609375, 0.9690284729003906, 0.9762153625488281, 0.9562911987304688, 0.9756851196289062, 0.9616737365722656, 0.9256782531738281, 0.9642181396484375, 0.9780807495117188, 0.9670524597167969, 0.95989990234375, 0.9767189025878906, 0.9686508178710938, 0.974822998046875, 0.9734001159667969, 0.9737510681152344, 0.9726943969726562, 0.9710311889648438, 0.9432296752929688, 0.9728736877441406, 0.9697303771972656, 0.9638862609863281, 0.9727325439453125, 0.9705467224121094, 0.9729232788085938, 0.9625091552734375, 0.9591140747070312, 0.9738426208496094, 0.98150634765625, 0.9712982177734375, 0.9817695617675781, 0.9725379943847656, 0.9681587219238281, 0.9807357788085938, 0.9716453552246094, 0.9647331237792969, 0.9676284790039062, 0.9297904968261719, 0.9700202941894531, 0.9825706481933594, 0.9727325439453125, 0.9764137268066406, 0.9702186584472656, 0.9771881103515625, 0.965606689453125, 0.96942138671875, 0.97845458984375, 0.952728271484375, 0.9807891845703125, 0.9693603515625, 0.9650421142578125, 0.9458084106445312, 0.9751815795898438, 0.9520492553710938, 0.9751930236816406, 0.977447509765625, 0.9759559631347656, 0.9650115966796875, 0.9747695922851562, 0.9809799194335938, 0.9777984619140625, 0.9556427001953125, 0.9713592529296875, 0.9834365844726562, 0.9602737426757812, 0.9731903076171875, 0.9694328308105469, 0.9741630554199219, 0.9756202697753906, 0.9754486083984375, 0.9778289794921875, 0.975372314453125, 0.9739112854003906, 0.9780349731445312, 0.9801445007324219, 0.96795654296875, 0.9778022766113281, 0.9483909606933594, 0.9666328430175781, 0.9683494567871094, 0.9813079833984375, 0.9762802124023438, 0.9704322814941406, 0.9632720947265625, 0.972503662109375, 0.9734725952148438, 0.9832382202148438, 0.9723091125488281, 0.9749183654785156, 0.9782218933105469, 0.9723358154296875, 0.9713211059570312, 0.9737663269042969, 0.9722747802734375, 0.9631080627441406, 0.9660873413085938, 0.9747848510742188, 0.9665374755859375, 0.9801216125488281, 0.9540176391601562, 0.9805259704589844, 0.9608230590820312, 0.9774169921875, 0.9684715270996094, 0.9558982849121094, 0.9731712341308594, 0.9736824035644531, 0.9659576416015625, 0.9668426513671875, 0.9825859069824219, 0.9684715270996094, 0.9587516784667969, 0.9760208129882812, 0.9758872985839844, 0.9702949523925781, 0.9735679626464844, 0.9581871032714844, 0.9615669250488281, 0.9734344482421875, 0.973724365234375, 0.9727096557617188, 0.9668006896972656, 0.9738693237304688, 0.9649696350097656, 0.9749908447265625, 0.9761543273925781, 0.9797821044921875, 0.9758415222167969, 0.9734039306640625, 0.9701881408691406, 0.9732170104980469, 0.9730186462402344, 0.9709320068359375, 0.9694061279296875, 0.9709014892578125, 0.9697761535644531, 0.9784278869628906, 0.9652938842773438, 0.97637939453125, 0.9783515930175781, 0.9553909301757812, 0.97314453125, 0.9717254638671875, 0.9709663391113281, 0.9711952209472656, 0.9689216613769531, 0.9737548828125, 0.9719772338867188, 0.9718017578125, 0.9716072082519531, 0.9707298278808594, 0.9732933044433594, 0.9707450866699219, 0.9710731506347656, 0.9767417907714844, 0.9734039306640625, 0.974853515625, 0.9736824035644531, 0.9648628234863281, 0.9768409729003906, 0.9751968383789062, 0.9667816162109375, 0.9733734130859375, 0.9681243896484375, 0.9685401916503906, 0.9808540344238281, 0.9733314514160156, 0.9781684875488281, 0.9596977233886719, 0.9720115661621094, 0.9774208068847656, 0.9510841369628906, 0.9686050415039062, 0.9760398864746094, 0.9762458801269531, 0.9607353210449219, 0.9719429016113281, 0.9779434204101562, 0.9664497375488281, 0.9685440063476562, 0.9647483825683594, 0.9704971313476562, 0.9728355407714844, 0.9737281799316406, 0.9771842956542969, 0.9608535766601562, 0.9706611633300781, 0.9759941101074219, 0.9766426086425781, 0.9754753112792969, 0.9689903259277344, 0.9810523986816406, 0.9664993286132812, 0.9749412536621094, 0.9768905639648438, 0.9633865356445312, 0.97406005859375, 0.9569511413574219, 0.9677810668945312, 0.96905517578125, 0.9682540893554688, 0.9802474975585938, 0.9697189331054688, 0.9678611755371094, 0.976318359375, 0.9705657958984375, 0.9739723205566406, 0.9707450866699219, 0.9599151611328125, 0.9676094055175781, 0.9691314697265625, 0.9772567749023438, 0.9696693420410156, 0.9756927490234375, 0.9752349853515625, 0.962982177734375]\n",
      "Average accuracy:0.9708\n"
     ]
    }
   ],
   "source": [
    "history_accuracy = []\n",
    "testing_loss_metric = MeanAveragePrecision()\n",
    "with torch.no_grad(): \n",
    "    # for data in test_data: \n",
    "    for idx_batch, (imagergb, labelmask, labelrgb) in enumerate(img_batch):\n",
    "        imagergb = imagergb.to(device)\n",
    "        labelmask = labelmask.to(device)\n",
    "        labelrgb = labelrgb.to(device)\n",
    "        \n",
    "        predicted_outputs = UNet(imagergb)\n",
    "        pred_class = torch.zeros((predicted_outputs.size()[0], predicted_outputs.size()[2], predicted_outputs.size()[3]))\n",
    "        for idx in range(0, predicted_outputs.size()[0]):\n",
    "            pred_class[idx] = torch.argmax(predicted_outputs[idx], dim=0).cpu().int()\n",
    "\n",
    "        pred_class = pred_class.unsqueeze(1).float()\n",
    "        labelmask = labelmask.unsqueeze(1).float()\n",
    "\n",
    "        acc_sum = (pred_class == labelmask).sum()\n",
    "        acc = float(acc_sum) / (labelmask.size()[0]*labelmask.size()[2]*labelmask.size()[3])\n",
    "        history_accuracy.append(acc)\n",
    "\n",
    "\n",
    "print(history_accuracy)\n",
    "print('Average accuracy:%.4f' % (sum(history_accuracy)/len(history_accuracy))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eleni\\Computer_Vision\\Computer-Vision-Project\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9528121948242188, 0.9704017639160156, 0.973480224609375, 0.9681129455566406, 0.95556640625, 0.9563674926757812, 0.9689674377441406, 0.9705238342285156, 0.9660148620605469, 0.9724159240722656, 0.9712562561035156, 0.964996337890625, 0.9597587585449219, 0.9572601318359375, 0.9643516540527344, 0.9689559936523438, 0.9746017456054688, 0.9644432067871094, 0.9555015563964844, 0.9666938781738281, 0.975067138671875, 0.9609718322753906, 0.9621124267578125, 0.9698066711425781, 0.9710922241210938, 0.9631233215332031, 0.9736366271972656, 0.9690322875976562, 0.9574432373046875, 0.9607505798339844, 0.9626998901367188, 0.9643516540527344, 0.966156005859375, 0.9637641906738281, 0.9666481018066406, 0.9750862121582031, 0.9529533386230469, 0.9664802551269531, 0.9672203063964844, 0.9649734497070312, 0.9667282104492188, 0.9659423828125, 0.9612808227539062, 0.9511337280273438, 0.9681472778320312, 0.9740104675292969, 0.9674186706542969, 0.9678497314453125, 0.9592094421386719, 0.9671134948730469, 0.9520645141601562, 0.9635200500488281, 0.9658660888671875, 0.9756622314453125, 0.9656333923339844, 0.9703559875488281, 0.9704055786132812, 0.9718971252441406, 0.9658355712890625, 0.9613037109375, 0.9716224670410156, 0.938018798828125, 0.9742317199707031, 0.966827392578125, 0.9678268432617188, 0.9549827575683594, 0.9795303344726562, 0.9370651245117188, 0.9674835205078125, 0.9543914794921875, 0.9695625305175781, 0.9635009765625, 0.9638137817382812, 0.9682235717773438, 0.9646530151367188, 0.9707679748535156, 0.9643325805664062, 0.9732131958007812, 0.9512100219726562, 0.9631729125976562, 0.9741287231445312, 0.9716529846191406, 0.9693603515625, 0.9751052856445312, 0.93756103515625, 0.9701957702636719, 0.9628067016601562, 0.9692153930664062, 0.9682426452636719, 0.9773216247558594, 0.9623336791992188, 0.946136474609375, 0.9751052856445312, 0.973724365234375, 0.9547004699707031, 0.9766807556152344, 0.9518814086914062, 0.9621162414550781, 0.92669677734375, 0.9570350646972656, 0.9672660827636719, 0.9495086669921875, 0.9692420959472656, 0.9664382934570312, 0.9652671813964844, 0.9646530151367188, 0.9727096557617188, 0.9755783081054688, 0.9627914428710938, 0.9678726196289062, 0.9729995727539062, 0.955047607421875, 0.9737014770507812, 0.96917724609375, 0.9660873413085938, 0.9651069641113281, 0.972686767578125, 0.9735031127929688, 0.9665565490722656, 0.9561042785644531, 0.9475898742675781, 0.96246337890625, 0.9689140319824219, 0.9590415954589844, 0.9671211242675781, 0.9676055908203125, 0.9633216857910156, 0.9349098205566406, 0.9701614379882812, 0.9576492309570312, 0.9520492553710938, 0.961578369140625, 0.9657173156738281, 0.957427978515625, 0.9681510925292969, 0.9614753723144531, 0.9693870544433594, 0.962127685546875, 0.969390869140625, 0.9745826721191406, 0.9806709289550781, 0.9604454040527344, 0.9531211853027344, 0.9718360900878906, 0.9573135375976562, 0.9567756652832031, 0.9631690979003906, 0.963104248046875, 0.9683151245117188, 0.9539604187011719, 0.9626731872558594, 0.9529037475585938, 0.967315673828125, 0.9671821594238281, 0.9713630676269531, 0.9647560119628906, 0.9608993530273438, 0.9578742980957031, 0.9713363647460938, 0.9637413024902344, 0.9410591125488281, 0.9703788757324219, 0.9694252014160156, 0.9639434814453125, 0.9300346374511719, 0.9701614379882812, 0.9600944519042969, 0.9295082092285156, 0.9610786437988281, 0.9593009948730469, 0.9716720581054688, 0.96893310546875, 0.9661293029785156, 0.97021484375, 0.9574966430664062, 0.9629325866699219, 0.9663047790527344, 0.9624099731445312, 0.958770751953125, 0.9724845886230469, 0.9668388366699219, 0.9534568786621094, 0.9705238342285156, 0.947235107421875, 0.9633903503417969, 0.9664573669433594, 0.9583015441894531, 0.9601058959960938, 0.9655799865722656, 0.9436607360839844, 0.9699440002441406, 0.9680519104003906, 0.9741439819335938, 0.9596977233886719, 0.9640960693359375, 0.9553565979003906, 0.9623298645019531, 0.9667129516601562, 0.9652214050292969, 0.9705924987792969, 0.9651603698730469, 0.9661827087402344, 0.963165283203125, 0.9715347290039062, 0.9453125, 0.95489501953125, 0.9639701843261719, 0.9668159484863281, 0.9628105163574219, 0.9524955749511719, 0.9649124145507812, 0.9642982482910156, 0.9691352844238281, 0.9763717651367188, 0.9689254760742188, 0.9674301147460938, 0.9732208251953125, 0.96917724609375, 0.96685791015625, 0.9766044616699219, 0.9724884033203125, 0.95684814453125, 0.9593544006347656, 0.9559440612792969, 0.9632148742675781, 0.9358100891113281, 0.9743118286132812, 0.9762306213378906, 0.9693756103515625, 0.9705848693847656, 0.9697189331054688, 0.9602584838867188, 0.9584007263183594, 0.9417076110839844, 0.9712028503417969, 0.974578857421875, 0.9643669128417969, 0.9745445251464844, 0.9495773315429688, 0.96661376953125, 0.945068359375, 0.9667243957519531, 0.9670181274414062, 0.9588813781738281, 0.9610023498535156, 0.9489021301269531, 0.973602294921875, 0.9586868286132812, 0.9632568359375, 0.9534339904785156, 0.9659957885742188, 0.9739990234375, 0.9674263000488281, 0.9720001220703125, 0.9613418579101562, 0.9473037719726562, 0.9786338806152344, 0.9337615966796875, 0.9685516357421875, 0.9721298217773438, 0.9673271179199219, 0.9703330993652344, 0.97576904296875, 0.9589462280273438, 0.9608268737792969, 0.9268760681152344, 0.9570541381835938, 0.9564247131347656, 0.9715538024902344, 0.9667167663574219, 0.9665184020996094, 0.9570121765136719, 0.9525260925292969, 0.9677505493164062, 0.9702377319335938, 0.9700393676757812, 0.9564361572265625, 0.963653564453125, 0.9619636535644531, 0.9686737060546875, 0.9627838134765625, 0.9734268188476562, 0.9566078186035156, 0.9583015441894531, 0.9709243774414062, 0.946990966796875, 0.9679107666015625, 0.9682235717773438, 0.9691314697265625, 0.9620361328125, 0.9723167419433594, 0.9769477844238281, 0.9666938781738281, 0.9540328979492188, 0.9754104614257812, 0.9606475830078125, 0.9719924926757812, 0.9629669189453125, 0.9717864990234375, 0.9625701904296875, 0.973358154296875, 0.9629592895507812, 0.9427261352539062, 0.9777603149414062, 0.9589653015136719, 0.9456520080566406, 0.9640846252441406, 0.9593467712402344, 0.9620170593261719, 0.97216796875, 0.9716033935546875, 0.9493141174316406, 0.9729728698730469, 0.9643173217773438, 0.9515190124511719, 0.975128173828125, 0.9602584838867188, 0.9573211669921875, 0.9654884338378906, 0.9699897766113281, 0.9662551879882812, 0.9694442749023438, 0.9670753479003906, 0.954498291015625, 0.9608879089355469, 0.9612159729003906, 0.9692573547363281, 0.9538230895996094, 0.9688911437988281, 0.9694862365722656, 0.9451675415039062, 0.9675750732421875, 0.9704093933105469, 0.9712181091308594, 0.9744796752929688, 0.9742622375488281, 0.9659538269042969, 0.9757347106933594, 0.9625015258789062, 0.9779777526855469, 0.9659957885742188, 0.9489021301269531, 0.9625167846679688, 0.9559097290039062, 0.96893310546875, 0.9609146118164062, 0.9672508239746094, 0.9554977416992188, 0.9634017944335938, 0.9599266052246094, 0.9799308776855469, 0.9734230041503906, 0.9733161926269531, 0.9691734313964844, 0.9773445129394531, 0.9526176452636719, 0.9650115966796875, 0.9687271118164062, 0.9636878967285156, 0.9558219909667969, 0.9408950805664062, 0.9649429321289062, 0.9662933349609375, 0.9492721557617188, 0.9724960327148438, 0.9712066650390625, 0.9551658630371094, 0.9699440002441406, 0.9553642272949219, 0.9638442993164062, 0.967315673828125, 0.9642464773995536]\n",
      "Average accuracy:0.9638\n"
     ]
    }
   ],
   "source": [
    "history_accuracy = []\n",
    "testing_loss_metric = MeanAveragePrecision()\n",
    "with torch.no_grad(): \n",
    "    # for data in test_data: \n",
    "    for idx_batch, (imagergb, labelmask, labelrgb) in enumerate(img_batch):\n",
    "        imagergb = imagergb.to(device)\n",
    "        labelmask = labelmask.to(device)\n",
    "        labelrgb = labelrgb.to(device)\n",
    "        \n",
    "        output1 = PreNet(imagergb)\n",
    "        output2 = F.pad(input=output1, pad=(2, 2, 2, 2))\n",
    "        predicted_outputs = UNet(output2)\n",
    "        pred_class = torch.zeros((predicted_outputs.size()[0], predicted_outputs.size()[2], predicted_outputs.size()[3]))\n",
    "        for idx in range(0, predicted_outputs.size()[0]):\n",
    "            pred_class[idx] = torch.argmax(predicted_outputs[idx], dim=0).cpu().int()\n",
    "\n",
    "        pred_class = pred_class.unsqueeze(1).float()\n",
    "        labelmask = labelmask.unsqueeze(1).float()\n",
    "\n",
    "        acc_sum = (pred_class == labelmask).sum()\n",
    "        acc = float(acc_sum) / (labelmask.size()[0]*labelmask.size()[2]*labelmask.size()[3])\n",
    "        history_accuracy.append(acc)\n",
    "\n",
    "\n",
    "print(history_accuracy)\n",
    "print('Average accuracy:%.4f' % (sum(history_accuracy)/len(history_accuracy))) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
