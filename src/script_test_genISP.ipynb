{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from dataset import CityscapesDataset\n",
    "from torchvision.datasets import Cityscapes\n",
    "import torch.nn.functional as F\n",
    "from GenISP import *\n",
    "from unet import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os \n",
    "from tqdm import trange\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eleni\\Computer_Vision\\Computer-Vision-Project\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ALL CREDITS TO https://github.com/goldbattle/pytorch_unet/blob/master/script_train.py FOR DATASET CLASS\n",
    "# path = \"/Users/eleni/Cityscapes/\"\n",
    "path = R\"C:\\Users\\eleni\\Cityscapes\"\n",
    "# path = open(r\"C:\\Users\\eleni\\Downloads\\leftImg8bit_trainvaltest\\cityscapes\"\n",
    "img_data = CityscapesDataset(path, split='train', mode='fine')\n",
    "test_data = CityscapesDataset(path, split='test', mode='fine')\n",
    "batch_size = 8\n",
    "img_batch = torch.utils.data.DataLoader(img_data, batch_size=batch_size, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and stuff\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(device)\n",
    "\n",
    "# Set model parameters\n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "in_c = 3\n",
    "\n",
    "# Create the model\n",
    "in_channels = 3 # Raw RGB image\n",
    "hidden_channels = [16, 32, 128, 16, 64, 3]\n",
    "out_features = 3 # GenISP output channels\n",
    "PreNet = GenISPV2( in_channels, hidden_channels, out_features)\n",
    "UNet = UNet(in_c, img_data.num_classes, 64)\n",
    "# Net = nn.Sequential(PreNet,UNet)\n",
    "torch.no_grad()\n",
    "PreNet.to(device)\n",
    "UNet.to(device)\n",
    "train_loss = []\n",
    "UNet.load_state_dict(torch.load(R\"C:\\Users\\eleni\\Computer_Vision\\Computer-Vision-Project\\model.pth\", map_location=torch.device('cpu')))\n",
    "PreNet.load_state_dict(torch.load(R\"C:\\Users\\eleni\\Computer_Vision\\Computer-Vision-Project\\prenet_model.pth\", map_location=torch.device('cpu')))\n",
    "# UNet.load_state_dict(torch.load(\"model.pth\"),map_location=torch.device('cpu'))\n",
    "# PreNet.load_state_dict(torch.load(\"prenet_model.pth\"))\n",
    "\n",
    "# Since our goal here is to segment, we will use the cross entropy loss function and 4 classes (background, road, sky, car)\n",
    "criterion = nn.CrossEntropyLoss() # for segment loss. could also be reconstruction loss, then it is N1 loss \n",
    "num_classes = img_data.num_classes\n",
    "print(num_classes)\n",
    "optimizer = torch.optim.Adam(PreNet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eleni\\Computer_Vision\\Computer-Vision-Project\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9528121948242188, 0.9704017639160156, 0.973480224609375, 0.9681129455566406, 0.95556640625, 0.9563674926757812, 0.9689674377441406, 0.9705238342285156, 0.9660148620605469, 0.9724159240722656, 0.9712562561035156, 0.964996337890625, 0.9597587585449219, 0.9572601318359375, 0.9643516540527344, 0.9689559936523438, 0.9746017456054688, 0.9644432067871094, 0.9555015563964844, 0.9666938781738281, 0.975067138671875, 0.9609718322753906, 0.9621124267578125, 0.9698066711425781, 0.9710922241210938, 0.9631233215332031, 0.9736366271972656, 0.9690322875976562, 0.9574432373046875, 0.9607505798339844, 0.9626998901367188, 0.9643516540527344, 0.966156005859375, 0.9637641906738281, 0.9666481018066406, 0.9750862121582031, 0.9529533386230469, 0.9664802551269531, 0.9672203063964844, 0.9649734497070312, 0.9667282104492188, 0.9659423828125, 0.9612808227539062, 0.9511337280273438, 0.9681472778320312, 0.9740104675292969, 0.9674186706542969, 0.9678497314453125, 0.9592094421386719, 0.9671134948730469, 0.9520645141601562, 0.9635200500488281, 0.9658660888671875, 0.9756622314453125, 0.9656333923339844, 0.9703559875488281, 0.9704055786132812, 0.9718971252441406, 0.9658355712890625, 0.9613037109375, 0.9716224670410156, 0.938018798828125, 0.9742317199707031, 0.966827392578125, 0.9678268432617188, 0.9549827575683594, 0.9795303344726562, 0.9370651245117188, 0.9674835205078125, 0.9543914794921875, 0.9695625305175781, 0.9635009765625, 0.9638137817382812, 0.9682235717773438, 0.9646530151367188, 0.9707679748535156, 0.9643325805664062, 0.9732131958007812, 0.9512100219726562, 0.9631729125976562, 0.9741287231445312, 0.9716529846191406, 0.9693603515625, 0.9751052856445312, 0.93756103515625, 0.9701957702636719, 0.9628067016601562, 0.9692153930664062, 0.9682426452636719, 0.9773216247558594, 0.9623336791992188, 0.946136474609375, 0.9751052856445312, 0.973724365234375, 0.9547004699707031, 0.9766807556152344, 0.9518814086914062, 0.9621162414550781, 0.92669677734375, 0.9570350646972656, 0.9672660827636719, 0.9495086669921875, 0.9692420959472656, 0.9664382934570312, 0.9652671813964844, 0.9646530151367188, 0.9727096557617188, 0.9755783081054688, 0.9627914428710938, 0.9678726196289062, 0.9729995727539062, 0.955047607421875, 0.9737014770507812, 0.96917724609375, 0.9660873413085938, 0.9651069641113281, 0.972686767578125, 0.9735031127929688, 0.9665565490722656, 0.9561042785644531, 0.9475898742675781, 0.96246337890625, 0.9689140319824219, 0.9590415954589844, 0.9671211242675781, 0.9676055908203125, 0.9633216857910156, 0.9349098205566406, 0.9701614379882812, 0.9576492309570312, 0.9520492553710938, 0.961578369140625, 0.9657173156738281, 0.957427978515625, 0.9681510925292969, 0.9614753723144531, 0.9693870544433594, 0.962127685546875, 0.969390869140625, 0.9745826721191406, 0.9806709289550781, 0.9604454040527344, 0.9531211853027344, 0.9718360900878906, 0.9573135375976562, 0.9567756652832031, 0.9631690979003906, 0.963104248046875, 0.9683151245117188, 0.9539604187011719, 0.9626731872558594, 0.9529037475585938, 0.967315673828125, 0.9671821594238281, 0.9713630676269531, 0.9647560119628906, 0.9608993530273438, 0.9578742980957031, 0.9713363647460938, 0.9637413024902344, 0.9410591125488281, 0.9703788757324219, 0.9694252014160156, 0.9639434814453125, 0.9300346374511719, 0.9701614379882812, 0.9600944519042969, 0.9295082092285156, 0.9610786437988281, 0.9593009948730469, 0.9716720581054688, 0.96893310546875, 0.9661293029785156, 0.97021484375, 0.9574966430664062, 0.9629325866699219, 0.9663047790527344, 0.9624099731445312, 0.958770751953125, 0.9724845886230469, 0.9668388366699219, 0.9534568786621094, 0.9705238342285156, 0.947235107421875, 0.9633903503417969, 0.9664573669433594, 0.9583015441894531, 0.9601058959960938, 0.9655799865722656, 0.9436607360839844, 0.9699440002441406, 0.9680519104003906, 0.9741439819335938, 0.9596977233886719, 0.9640960693359375, 0.9553565979003906, 0.9623298645019531, 0.9667129516601562, 0.9652214050292969, 0.9705924987792969, 0.9651603698730469, 0.9661827087402344, 0.963165283203125, 0.9715347290039062, 0.9453125, 0.95489501953125, 0.9639701843261719, 0.9668159484863281, 0.9628105163574219, 0.9524955749511719, 0.9649124145507812, 0.9642982482910156, 0.9691352844238281, 0.9763717651367188, 0.9689254760742188, 0.9674301147460938, 0.9732208251953125, 0.96917724609375, 0.96685791015625, 0.9766044616699219, 0.9724884033203125, 0.95684814453125, 0.9593544006347656, 0.9559440612792969, 0.9632148742675781, 0.9358100891113281, 0.9743118286132812, 0.9762306213378906, 0.9693756103515625, 0.9705848693847656, 0.9697189331054688, 0.9602584838867188, 0.9584007263183594, 0.9417076110839844, 0.9712028503417969, 0.974578857421875, 0.9643669128417969, 0.9745445251464844, 0.9495773315429688, 0.96661376953125, 0.945068359375, 0.9667243957519531, 0.9670181274414062, 0.9588813781738281, 0.9610023498535156, 0.9489021301269531, 0.973602294921875, 0.9586868286132812, 0.9632568359375, 0.9534339904785156, 0.9659957885742188, 0.9739990234375, 0.9674263000488281, 0.9720001220703125, 0.9613418579101562, 0.9473037719726562, 0.9786338806152344, 0.9337615966796875, 0.9685516357421875, 0.9721298217773438, 0.9673271179199219, 0.9703330993652344, 0.97576904296875, 0.9589462280273438, 0.9608268737792969, 0.9268760681152344, 0.9570541381835938, 0.9564247131347656, 0.9715538024902344, 0.9667167663574219, 0.9665184020996094, 0.9570121765136719, 0.9525260925292969, 0.9677505493164062, 0.9702377319335938, 0.9700393676757812, 0.9564361572265625, 0.963653564453125, 0.9619636535644531, 0.9686737060546875, 0.9627838134765625, 0.9734268188476562, 0.9566078186035156, 0.9583015441894531, 0.9709243774414062, 0.946990966796875, 0.9679107666015625, 0.9682235717773438, 0.9691314697265625, 0.9620361328125, 0.9723167419433594, 0.9769477844238281, 0.9666938781738281, 0.9540328979492188, 0.9754104614257812, 0.9606475830078125, 0.9719924926757812, 0.9629669189453125, 0.9717864990234375, 0.9625701904296875, 0.973358154296875, 0.9629592895507812, 0.9427261352539062, 0.9777603149414062, 0.9589653015136719, 0.9456520080566406, 0.9640846252441406, 0.9593467712402344, 0.9620170593261719, 0.97216796875, 0.9716033935546875, 0.9493141174316406, 0.9729728698730469, 0.9643173217773438, 0.9515190124511719, 0.975128173828125, 0.9602584838867188, 0.9573211669921875, 0.9654884338378906, 0.9699897766113281, 0.9662551879882812, 0.9694442749023438, 0.9670753479003906, 0.954498291015625, 0.9608879089355469, 0.9612159729003906, 0.9692573547363281, 0.9538230895996094, 0.9688911437988281, 0.9694862365722656, 0.9451675415039062, 0.9675750732421875, 0.9704093933105469, 0.9712181091308594, 0.9744796752929688, 0.9742622375488281, 0.9659538269042969, 0.9757347106933594, 0.9625015258789062, 0.9779777526855469, 0.9659957885742188, 0.9489021301269531, 0.9625167846679688, 0.9559097290039062, 0.96893310546875, 0.9609146118164062, 0.9672508239746094, 0.9554977416992188, 0.9634017944335938, 0.9599266052246094, 0.9799308776855469, 0.9734230041503906, 0.9733161926269531, 0.9691734313964844, 0.9773445129394531, 0.9526176452636719, 0.9650115966796875, 0.9687271118164062, 0.9636878967285156, 0.9558219909667969, 0.9408950805664062, 0.9649429321289062, 0.9662933349609375, 0.9492721557617188, 0.9724960327148438, 0.9712066650390625, 0.9551658630371094, 0.9699440002441406, 0.9553642272949219, 0.9638442993164062, 0.967315673828125, 0.9642464773995536]\n",
      "Average accuracy:0.9638\n"
     ]
    }
   ],
   "source": [
    "# running_accuracy = 0 \n",
    "# total = 0 \n",
    "# AP_50_loss = 0\n",
    "# AP_75_loss = 0\n",
    "# AP_loss = 0\n",
    "# -------------------------------\n",
    "# pixel_acc = 0.0\n",
    "# dice=0.0\n",
    "# precision = 0.0\n",
    "# specificity = 0.0\n",
    "# recall = 0.0\n",
    "# --------------------------------\n",
    "# dice_coeff = 0.0\n",
    "# num_samples = 0\n",
    "# --------------------------------\n",
    "history_accuracy = []\n",
    "testing_loss_metric = MeanAveragePrecision()\n",
    "with torch.no_grad(): \n",
    "    # for data in test_data: \n",
    "    for idx_batch, (imagergb, labelmask, labelrgb) in enumerate(img_batch):\n",
    "        # inputs, outputs = data \n",
    "        # outputs = outputs.to(torch.float32)\n",
    "        imagergb = imagergb.to(device)\n",
    "        labelmask = labelmask.to(device)\n",
    "        labelrgb = labelrgb.to(device)\n",
    "        \n",
    "        output1 = PreNet(imagergb)\n",
    "        output2 = F.pad(input=output1, pad=(2, 2, 2, 2))\n",
    "        predicted_outputs = UNet(output2)\n",
    "        pred_class = torch.zeros((predicted_outputs.size()[0], predicted_outputs.size()[2], predicted_outputs.size()[3]))\n",
    "        for idx in range(0, predicted_outputs.size()[0]):\n",
    "            pred_class[idx] = torch.argmax(predicted_outputs[idx], dim=0).cpu().int()\n",
    "\n",
    "        pred_class = pred_class.unsqueeze(1).float()\n",
    "        labelmask = labelmask.unsqueeze(1).float()\n",
    "\n",
    "        acc_sum = (pred_class == labelmask).sum()\n",
    "        acc = float(acc_sum) / (labelmask.size()[0]*labelmask.size()[2]*labelmask.size()[3])\n",
    "        history_accuracy.append(acc)\n",
    "# ----------------------------------------\n",
    "#         predicted_outputs.resize_(8, 3, 128, 256)\n",
    "#         predicted = torch.argmax(predicted_outputs, dim=1)\n",
    "#         print(predicted.size())\n",
    "#         intersection = torch.logical_and(predicted, labelrgb).sum().item()\n",
    "#         union = predicted.sum().item() + labelrgb.sum().item()\n",
    "\n",
    "#         dice_coeff += (2 * intersection) / (union + 1e-8)\n",
    "#         num_samples += 1\n",
    "#         print(dice_coeff)\n",
    "        \n",
    "\n",
    "# average_dice_coeff = dice_coeff / num_samples\n",
    "print(history_accuracy)\n",
    "print('Average accuracy:%.4f' % (sum(history_accuracy)/len(history_accuracy))) \n",
    "\n",
    "# -------------------------------------------------\n",
    "        # predicted_outputs.resize_(8, 3, 128, 256)\n",
    "        # p, d, pr, sp, r = calculate_overlap_metrics(labelrgb,predicted_outputs)\n",
    "        # pixel_acc += p\n",
    "        # dice += d\n",
    "        # precision += pr\n",
    "        # specificity += sp\n",
    "        # recall += r\n",
    "        # print(calculate_overlap_metrics(labelrgb,predicted_outputs))\n",
    "        # -------------------------------------\n",
    "        # testing_loss_metric.update(predicted_outputs, labelrgb)\n",
    "        # loss = testing_loss_metric.compute()\n",
    "        # AP_50_loss += loss[\"map_50\"]\n",
    "        # AP_75_loss += loss[\"map_75\"]\n",
    "        # AP_loss += loss[\"map\"]\n",
    "        # -----------------------------------------\n",
    "        # # print('eleni')\n",
    "        # _, predicted = torch.max(predicted_outputs, 1) \n",
    "        # total += labelrgb.size(0) \n",
    "        # running_accuracy += (predicted == labelrgb).sum().item() \n",
    "                \n",
    "# print(average_dice_coeff)\n",
    "    # print(pixel_acc)\n",
    "    # print(dice)\n",
    "    # print(precision)\n",
    "    # print(specificity)\n",
    "    # print(recall)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
